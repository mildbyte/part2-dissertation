\documentclass[12pt,a4]{article}
\usepackage{hyperref}
\usepackage{color}
\usepackage{fullpage}
\begin{document}

\vfil


\begin{flushright}
\large{A. I\v{s}kovs}\\
\texttt{ai280}\\
Trinity College
\end{flushright}

\vspace*{\fill}
\begin{center}{\large Computer Science Tripos, Part II Project Proposal}

\vspace{0.3in}
\textbf{\center{\Large Predicting drug-pathway interactions using the Correlated Topic Model}}

\vspace{0.4in}
%\today
\centerline{\large October 24, 2014}
\end{center}
\vspace*{\fill}

\vfil

%
%\noindent{\bf Project Originator:} N. Pratanwanich
%\vspace{0.4in}
%
%\noindent
%{\bf Project Supervisor:} N. Pratanwanich\\
%
%\noindent
%{\bf Signature:}
%\vspace{0.2in}
%
%\noindent
%{\bf Director of Studies:} Dr A. C. Norman\\
%
%\noindent
%{\bf Signature:}
%\vspace{0.2in}
% 
%\noindent
%{\bf Project Overseers:} Dr~A.~V.~S.~Madhavapeddy  \& Dr~S.~H.~Teuffel\\
%
%\noindent
%{\bf Signatures:}

\pagebreak

% Main document

\section*{Introduction}

Latent Dirichlet Allocation\cite{Blei} is a bag-of-words topic modelling algorithm that treats documents in a corpus as being generated by picking a set of topics and then drawing words from those topics. Using a sampling method, the posterior distributions of unknown variables (probability distributions of topics and words within topics) are inferred. This allows, for an arbitrary document, to infer a probability distribution of topics which it is about.

The 2014 paper\cite{Pratanwanich2014} by Naruemon Pratanwanich and Pietro Lio describes a model based on this algorithm to predict drug-pathway relationships: differential gene expression profiles of drug treatments are treated as documents, pathways where genes are functionally grouped as topics and genes as words. The model had to be augmented with priors: the pathway-gene relationships for some pathways are already known. In the end, this method provided better predictive results of pathways activated by certain drugs than the state-of-the art methods.

A problem with Latent Dirichlet Allocation and hence this approach is that it assumes that the topics (or pathways) are uncorrelated. This assumption is usually not true: a document about Computer Science is more likely to be about Mathematics than, say, Geology. Another 2014 paper\cite{C4MB00014E} by Naruemon Pratanwanich and Pietro Lio presents a model that assumes pathway crosstalk (correlation). It uses matrix factorisation together with this assumption to predict pathway responsiveness for drugs that affect multiple pathways and finds that using the correlation assumption results in a better fit to the gene expression data.

The Correlated Topic Model\cite{2007} was proposed as an improvement on the Latent Dirichlet Allocation: instead of using the Dirichlet distribution to model the relative proportions of topics in a document, it uses the logistic normal distribution. The parameters of the distribution are the mean and the covariance matrix, which allows for modelling correlations between topics. The original paper\cite{2007} uses a deterministic approach called Variational EM (Expectation Maximization) to train this model (due to the non-conjugacy of the logistic normal, the Markov Chain Monte Carlo approach with Gibbs Sampling is intractable). The Correlated Topic Model fit the sample corpus (a set of articles from \textit{Science}) better than LDA\cite{2007}.

Since the continuous matrix factorization model that assumed pathway correlation\cite{C4MB00014E} and the discrete LDA method\cite{Pratanwanich2014} outperform matrix factorization without correlations, it is suspected that adapting the CTM to the problem of prediction of pathway responsiveness to drug treatment would yield another improvement in predictive power.

The aim of this project is to implement and test such a model. In addition to reusing the Bayesian network from the CTM, the model will be augmented with a prior distribution of known gene-pathway relationships. The equations for training the model will be derived, based on the existing CTM equations for Variational EM\cite{2007}. The model will be tested on a synthetic corpus of drug gene expression data for model verification and then on publically available datasets (CMAP\cite{CMap} for gene expression data and KEGG\cite{KEGG} for pathway data). The results will be compared with the latent variables inferred by the LDA approach. While it's not possible to guarantee that the model will perform better in every case, the results will be investigated to see in which cases the LDA approach performs better than CTM and vice versa.

\section*{Starting Point}

I know Python and have some experience in working with the SciPy stack. Python has lots of facilities for data processing and while it is an interpreted language, NumPy uses native LAPACK/BLAS (linear algebra) libraries as a backend. This means that the performance of the linear algebra routines is on par with C.

Since this project uses some concepts from the Artificial Intelligence II course (see Key Concepts), I will have to familiarise myself with the course before it begins.

There exists an R package for the Correlated Topic Model, as well as a C implementation of the Model written by the authors of the original paper (\url{https://www.cs.princeton.edu/~blei/ctm-c/}). I will however only be able to use those as a reference at most: adding priors to the model will change its implementation. On the other hand, the C implementation has a way to perform posterior inference (prediction) on new documents, which can help with one of the extensions to the project.

\section*{Substance and Structure of the Project}

\subsection*{Key Concepts}

The key concepts in this project are drawn from the Artificial Intelligence II course, including topics such as Bayesian Networks and inference, methods for training and evaluating classifiers and probability distribution transformation. This project, however, will go beyond the scope of the course, touching on Variational Inference, conjugate and non-conjugate distributions and Expectation Maximization.

\subsection*{Major Work Items}

\subsubsection*{Required Reading and Research}

\begin{itemize}
\item Artificial Intelligence II Lecture notes: introduction to Bayesian networks and inference
\item Chapters 9 and 10 of Christopher Bishop's book\cite{Bishop:2006:PRM:1162264} on computer vision models: Mixture models (of which LDA and CTM are variations) and the Expectation Maximization algorithm, as well as Variational Inference.
\item ``Build, Compute, Critique, Repeat: Data Analysis with Latent Variable Models"\cite{doi:10.1146/annurev-statistics-022513-115657}, an introduction to latent variable models that describes how to train the models using mean-field Variational Inference, use them to perform predictions and evaluate them.
\item The original LDA\cite{Blei} and CTM\cite{2007} papers to understand the structures of the respective generative probabilistic models and their inference based on Variational EM.
\item Naruemon Pratanwanich and Pietro Lio's paper\cite{Pratanwanich2014}  on adapting LDA in order to familiarize myself with how microarray data is preprocessed into a pseudo-drug ``document", how the gene-pathway membership priors are incorporated into the model, as well as the ranking method used to evaluate its performance.

I will also need to get a better understanding of the biological context around the project, including topics such as genes, pathways, how the drug gene expression data is obtained using microarrays and how it has to be preprocessed.

\end{itemize}

\subsubsection*{Developing a Model}

The classic Expectation Maximization algorithm consists of two steps:

\begin{itemize}
\item \textbf{E}: Calculate the expected value of the log likelihood of seeing the latent variables given the observed variables and the model parameters.
\item \textbf{M}: Maximize this function by updating the model parameters.
\end{itemize}

The CTM paper\cite{2007} applies an algorithm called Variational EM that, using a variational distribution, approximates the posterior distributions of latent variables with respect to the model parameters in the \textbf{E} step and estimates the model parameters with respect to variational parameters in the \textbf{M} step.

I have to understand and alter this algorithm in order add prior information on gene-pathway memberships to the model.

\subsubsection*{Implementation and Testing}

In Python, implement:

\begin{itemize}
\item The software to generate synthetic data for model verification and to preprocess real datasets.
\item The actual Correlated Topic Model that incorporates the priors.
\end{itemize}

During development, the model will be tested using toy datasets of synthetic gene expression and pathway data. Using the real datasets, it will then be compared against the LDA method (see Evaluation and Success Criteria).

\subsubsection*{Fallback Plan}
In case I fail to incorporate priors into the Correlated Topic Model, I will use one of the available libraries for fitting the CTM and instead work on processing the gene data into a drug "document" that can be understood by the library, as well as developing a way to predict pathway distributions for a previously unseen drug. The evaluation procedure in that case will have to be changed, since not having priors in the model will mean that the pathways will be completely arbitrary and will not correspond to real drug data.

Another option is using a less complicated algorithm such as Random Forests\cite{Breiman:2001:RF:570181.570182} that has high predictive power on drug-pathway perturbations\cite{Riddick15012011}. In that case the setting will be that of supervised learning, where the training data are known pairs of drugs and pathways and the features are the differential gene expression data. However, in this case I won't be able to test the hypothesis of pathway correlation and how the data are generated as I would be able to by using the aforementioned generative probabilistic models.

\section*{Evaluation and Success Criteria}

The \textbf{main criterion} for success is a working implementation of the model as per its specification. This will be evaluated as follows:
\begin{itemize} 
\item Generate a toy dataset of random pathways, pathway-gene memberships (a probability distribution over genes) and drug-pathway interactions (a probability distribution over pathways)
\item Use that to generate a corpus of gene expression data as per how the Correlated Topic Model assumes it's generated.
\item Together with a random subset of the pathway-gene membership data to serve as a prior, use the model to infer the latent variables and compare them with the ones that generated the dataset.
\end{itemize}

Note that the successful achievement of this objective does not imply the model has any predictive power for real drug data. It only verifies that the model works as expected and there are no errors in the implementation.

The comparison with the LDA method can be performed in two ways. Firstly, the model can be run on the CMAP/KEGG datasets to infer the latent variables. One of these is the per-drug pathway distribution, which can then be compared with the distribution inferred by the LDA method based on the accuracy on the reference data. It is possible that the developed model will perform better on some drugs than LDA and worse on others. This will be investigated in order to know which types of drugs it's better to use one model over another.

In case \textbf{the extension goal} of having a method for predicting pathways activated by certain drugs has been reached, the model will be tested on reference data (known drug-pathway relationships). This will be done by performing the predictions for known drugs and then using the pathway ranking metric described in\cite{Pratanwanich2014}.


\section*{Extensions}
The original CTM paper does not explicitly mention how the CTM can be used to calculate the topic proportions for a new document. One extension would hence involve performing that, in which case it will be possible to better compare the performance of the model with the LDA approach using the ranking method described in the paper\cite{Pratanwanich2014} (without this extension, we can still compare the results by considering the latent variables that both models inferred from the data).

Yet another extension can be making an interface for the trained model in order to create a stand-alone tool that researchers can use.

There are multiple other priors that can be incorporated into the model to improve predictive power, for example, the correlation between drugs: the molecular structure of a drug can be treated as a set of functional groups and so drugs with similar sets of functional groups are likely to have similar effects.

In addition, the Variational EM approach only gives a maximum likelihood estimate of the model parameters. The model could be given a full Bayesian treatment by using hierarchical Bayesian modelling on the model parameter. This may help solve the known problem of singularity of the covariance matrix when there is a large number of topics\cite{Masada:2013:RIC:2525761.2525819}.

\section*{Resource Declaration}

The data for pathways will be taken from the KEGG\cite{KEGG} database and the drug gene expression data will be taken from CMAP\cite{CMap}, which are publically available.

I will be using my own laptop for main development, a quad-core machine with Windows and ArchLinux on it. The source code will be kept in a Git repository which will be regularly pushed to GitHub, as well as to my MCS filespace. As the KEGG and CMAP data is publically available, and the code used to read and prepare it for processing will be backed up, I do not expect to require to make backups of the actual datasets.

\section*{Timetable: Workplan and Milestones to be achieved.}

Planned starting date is 27/10/2014.

\begin{enumerate}

\item {\bf 27/10/2014 -- 09/11/2014} 

Perform required reading and research (as per the Major Work Items section). Set up the outline for the dissertation.

\item {\bf 10/11/2014 -- 23/11/2014} 

Modify the model and adapt the equations used for inference to incorporate priors. Start writing the Method section of the dissertation.

\item {\bf 24/11/2014 -- 11/01/2015} 

Implement the model in Python. Generate a small toy dataset to verify the implementation. Debug the code.

\item {\bf 12/01/2015 -- 25/01/2015}

Work on optimizing and debugging the code so that it can run in reasonable time on a large toy dataset. Write the progress report. Continue writing the Method and start writing the Evaluation sections of the dissertation.

\textbf{Milestone:} a working implementation of the modified model. 

\item {\bf 26/01/2015 -- 08/02/2015} 

Submit the progress report. Rehearse and deliver a presentation to the overseeing group.

Obtain and explore the KEGG and the CMAP datasets. Preprocess the data into a suitable format for the model.

\item {\bf 09/02/2015 -- 22/02/2015}

Derive the equations required for prediction of pathways that are perturbed given a new drug (extension 1).

\item {\bf 23/02/2015 -- 08/03/2015}

Use the CMAP/KEGG datasets to train the model and evaluate it against the performance of the LDA model.

\item {\bf 09/03/2015 -- 19/04/2015}

Work on the remaining extensions to the project (such as packaging the model as a stand-alone tool) and continue writing the dissertation.

\item {\bf 20/04/2015 -- 03/05/2015}

Finish writing a draft dissertation. Submit to the Director of Studies and the Supervisor for review, allowing 2 weeks to read the draft.

\item {\bf 04/05/2015 -- 10/05/2015}

Incorporate comments from the reviews. Submit the dissertation.

\end{enumerate}

\bibliographystyle{unsrt}
\bibliography{bibliography}

\end{document}