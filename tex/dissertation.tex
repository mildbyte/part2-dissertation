\documentclass[12pt,a4paper,twoside,openright]{report}
\usepackage[pdfborder={0 0 0}]{hyperref}    % turns references into hyperlinks
\usepackage[margin=25mm]{geometry}  % adjusts page layout
\usepackage{graphicx}  % allows inclusion of PDF, PNG and JPG images
\usepackage{verbatim}
\usepackage{docmute}   % only needed to allow inclusion of proposal.tex
\usepackage[noend]{algpseudocode}
\usepackage[]{algorithm}
\usepackage{amsmath}
\usepackage{mathtools}
\usepackage{enumitem}
\newcommand{\var}[1]{{\operatorname{\mathit{#1}}}}

\raggedbottom                           % try to avoid widows and orphans
\sloppy
\clubpenalty1000%
\widowpenalty1000%

\renewcommand{\baselinestretch}{1.1}    % adjust line spacing to make
                                        % more readable

\begin{document}

\bibliographystyle{plain}


%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
% Title


\pagestyle{empty}

\rightline{\LARGE \textbf{Artjoms Iskovs}}

\vspace*{60mm}
\begin{center}
\Huge
\textbf{Predicting drug-pathway interactions using the Correlated Topic Model} \\[5mm]
Computer Science Tripos -- Part II \\[5mm]
Trinity College \\[5mm]
\today  % today's date
\end{center}

%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
% Proforma, table of contents and list of figures

\pagestyle{plain}

\chapter*{Proforma}

{\large
\begin{tabular}{ll}
Name:               & \bf Artjoms Iskovs                       \\
College:            & \bf Trinity College                     \\
Project Title:     & \bf Predicting drug-pathway interactions using the Correlated Topic Model\\
Examination:        & \bf Computer Science Tripos -- Part II, July 2015  \\
Word Count:         & \bf ??\\
Project Originator: & N.~Pratanwanich                    \\
Supervisor:         & N.~Pratanwanich                    \\ 
\end{tabular}
}

\section*{Original Aims of the Project}
Implement the Correlated Topic Model for the problem of inferring pathways from drug gene expression data. Find a way to enforce gene-pathway membership priors so that the recovered topic structure can be related to the real data.

\section*{Work Completed}

\section*{Special Difficulties}
None
 
\newpage
\section*{Declaration}

I, Artjoms Iskovs of Trinity College, being a candidate for Part II of the Computer
Science Tripos, hereby declare that this dissertation and the work described in it are my own work,
unaided except as may be specified below, and that the dissertation does not contain material that has already been used to any substantial
extent for a comparable purpose.

\bigskip
\leftline{Signed [signature]}

\medskip
\leftline{Date [date]}

\tableofcontents

\listoffigures

\newpage
\section*{Acknowledgements}


%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
% now for the chapters

\pagestyle{headings}

\chapter{Introduction}

\section{Genes and gene expression data}

Drugs are often investigated by using gene expression microarray analysis: a matrix of various samples of DNA is treated with different drugs and their expression is compared with the baseline expression.

\section{Pathways}

Functionally similar genes are grouped into pathways.

\section{Topic models}

Topic models come from the realm of natural language processing and are bag-of-words models that allow for exploratory data analysis on collections of documents. They assume that documents in a corpus are generated by first drawing a distribution of topics for every document, then picking words for a document by first choosing a topic from this distribution and then sampling a word from that distribution.

Using topic models in order to predict pathways affected by drugs has many benefits. Firstly, pathways that a new drug is likely to affect can be predicted without having to perform tests on real people, which means that useless drugs can be rejected quicker. In addition, this kind of analysis can allow researchers to find new uses (affected pathways) for existing drugs. Finally, the inferred correlation matrix can provide useful insights into relationships between pathways (such as disease comorbidity).

\section{Using Latent Dirichlet Allocation for gene-pathway relationship prediction}

\chapter{Preparation}

\section{Performed reading}

\section{Setting up the development environment}

\chapter{Implementation}

\section{Training of the Correlated Topic Model}

This section sets up the notation used throughout the whole dissertation and derives the training and the inference process for the Correlated Topic Model. The training process was mostly derived from Blei's original paper, however I provide some additions here so that the derivation is easier to understand. The derivation of the inference process is mine, however I used Blei's CTM implementation in C to verify my method.

\begin{tabular}{| l | l |}
\hline
Variable & Description \\
\hline
$K$ & Number of topics in the corpus \\
$D$ & Number of documents in the corpus \\
$V$ & Number of words in the corpus (vocabulary size) \\
$w_n$ & Nth word in the document \\
$c_n$ & Number of times the nth word occurs in the document \\
$\mu$ & Mean of the normal distribution of topics in the corpus \\
$\Sigma$ & Covariance matrix of the normal distribution of topics in the corpus \\
$\beta$ & $K \times V$ dimensional matrix of topic-word distributions \\
$\eta$ & Drawn topic distribution for a certain document \\
$z$ & Drawn topic assignments for each word in the document \\
\hline
\end{tabular}

\subsection{Deriving the likelihood bound on the corpus}

We aim to find the model parameters that maximise the likelihood bound on the whole corpus. In other words, what are the model parameters that we need to set so that the corpus we're inspecting is the most likely one?

First of all, given a document and the global model parameters, consider the distribution of the possible distributions of topics and the topic assignments:

\begin{equation}
p(\eta, z | w, \beta, \mu, \Sigma)
\end{equation}

Using Bayes' Theorem (and conditioning on the global model parameters $\beta, \mu, \Sigma$ throughout), this can be rewritten as 

\begin{equation}
\frac{p(\eta, z | \beta, \mu, \Sigma) p(w | \eta, z, \beta, \mu, \Sigma)}{p(w | \beta, \mu, \Sigma)}
\end{equation}

First, consider, $p(w | \eta, z, \beta, \mu, \Sigma)$, the probability of the document given all the model parameters and the topic assignments. Since all words in the document are independent of each other, this can be rewritten as a product:

\begin{align}
p(w | \eta, z, \beta, \mu, \Sigma) &= \prod\limits_{n=1}^N p(w_n | z_n, \eta, \beta, \mu, \Sigma) p(z_n | \eta, \beta, \mu, \Sigma)\\
& = \prod\limits_{n=1}^N p(w_n | z_n, \beta) p(z_n | \eta)
\end{align}

By inspecting the Bayesian plate notation of the model, it can be seen that $w_n$ is only dependent on the topic assigment $z_n$ and the per-topic word distribution $\beta$, hence all of the other variables we are conditioning on can be removed from consideration. Similarly with $z_n$: it's only dependent on $\eta$.

The denominator is the probability of the document given the global model parameters and it has to be calculated by marginalising over all possible distributions of topics and topic assignments:

\begin{align}
p(w | \beta, \mu, \Sigma) & = \int p(w | \eta, \beta) p(\eta | \mu, \Sigma) d\eta \\
& =\int p(\eta | \mu, \Sigma)  \prod\limits_{n=1}^N p(w_n | \eta, \beta) d\eta \\
& =\int p(\eta | \mu, \Sigma)  \prod\limits_{n=1}^N \sum\limits_{z_n=1}^K p(z_n | \eta) p(w_n | z_n, \beta) d\eta
\end{align}

In this expansion, we first integrate over all possible distributions of topics $\eta$, then the integrand is expanded to be a product over all words in the document and then, finally, we integrate over all possible topic assignments (since the integral is discrete, it turns into a sum). As in the previous derivation, due to the independence assumptions, we liberally add and remove the variables we are conditioning on to simplify the expression.

The final expression, as it appears in Blei's paper, is

\begin{equation}
\frac{p(\eta | \mu, \Sigma) \prod\limits_{n=1}^N p(w_n | z_n, \beta) p(z_n | \eta)}{\int p(\eta | \mu, \Sigma)  \prod\limits_{n=1}^N \sum\limits_{z_n=1}^K p(z_n | \eta) p(w_n | z_n, \beta) d\eta}
\end{equation}

There are two problems with this expression's denominator (the marginal probability of the document). First, it contains a summation over $K$ possible values of the topic of the nth word in the document, $z_n$, which is inside a product over the $N$ words in the document, thus the integrand will contain $K^N$ terms: intractable with the sizes of the data used in this project. Worse even, the distributions $p(z_n | \eta)$ and $p(\eta | \mu, \Sigma)$ are not conjugate to each other, hence the integral over $\eta$ cannot be computed analytically.

\subsection{Using variational methods to approximate the likelihood bound}

To solve this problem, Blei uses variational methods, which involve approximating the posterior distribution with a simpler one, called the variational distribution. The parameters of the distribution are fit to the true posterior (by minimizing the Kullback-Leibner divergence) and the variational distribution is used as a substitute for the posterior.

The definition of the distribution is

\begin{equation}
q(\eta, z | \lambda, \nu, \phi) = \prod\limits_{i=1}^K q(\eta_i|\lambda_i, \nu_i^2) \prod\limits_{n=1}^N q(z_n | \phi_n)
\end{equation}

Here, the variational distribution of the per-document topic distribution $\eta$ is $K$ independent normal variables with means $\lambda$ and standard deviations $\nu$.

Minimizing the Kullback-Leibner divergence between the variational distribution and the true posterior is the same as optimizing the parameters of the variational distribution so that it maximizes the probability of the document. This is equivalent to maximizing the log probability of the document, which can be bounded as

\begin{align}
\log p(w | \beta, \mu, \Sigma) & \geq E_q[\log p(\eta|\mu, \Sigma)] \\
& + \sum\limits_{n=1}^N E_q[\log p(z_n | \eta)] \\
& + \sum\limits_{n=1}^N E_q[\log p(w_n | z_n, \beta)] \\
& + \mathit{H}(q)
\end{align}

Here, the expectation is taken with respect to the relevant variational distributions $q$ and $\mathit{H}(q)$ is the entropy of the variational distribution.

The components of the likelihood bound are then rewritten in terms of the variational parameters $\lambda, \nu, \phi$:

\begin{align}
E_q[\log p(\eta|\mu, \Sigma)] & = \frac{1}{2} \log |\Sigma^{-1}| - \frac{K}{2} \log 2 \pi - \frac{1}{2}E_q[(\eta - \mu)^T\Sigma^{-1}(\eta - \mu)] \\
& = \frac{1}{2} \log |\Sigma^{-1}| - \frac{K}{2} \log 2 \pi - \frac{1}{2}(\mathit{Tr}(\mathit{diag}(\nu^2)\Sigma^{-1} + (\lambda - \mu)^T\Sigma^{-1}(\lambda - \mu))
\end{align}

\begin{align}
E_q[\log p(z_n | \eta)] & = E_q[\eta^Tz_n] - E_q[\log \sum\limits_{i=1}^K e^{\eta_i}] \\
& \geq E_q[\eta^Tz_n] - \zeta^{-1}(\sum\limits_{i=1}^KE_q[e^{\eta_i}] + 1 - \log\zeta \\
& = \sum\limits_{i=1}^K\lambda_i\phi_{n, i} - \zeta^{-1}(\sum\limits_{i=1}^Ke^{\lambda_i + \nu_i^2 / 2}) + 1 - \log\zeta
\end{align}

Here, the term $E_q[\log \sum\limits_{i=1}^K e^{\eta_i}]$ was bound by a Taylor expansion, introducing an extra parameter $\zeta$.

\begin{equation}
E_q[\log p(w_n | z_n, \beta)] = \sum\limits_{i=1}^K\phi_{n,i}\log\beta_{i, w_n}
\end{equation}

\begin{equation}
\mathit{H}(q) = \sum\limits_{i=1}^K\frac{1}{2}(\log\nu_i^2 + \log 2 \pi + 1) - \sum\limits_{n=1}^N\sum\limits_{i=1}^K\phi_{n,i}\log\phi_{n,i}
\end{equation}

\subsection{Maximising the likelihood bound}

Finally, the optimisation of the likelihood bound happens by iteratively maximising it with respect to each one of the variational parameters.

First, the maximisation of the bound with respect to $\zeta$ can be performed analytically:

\begin{equation}
\hat\zeta = \sum\limits_{i=1}^Ke^{\lambda_i + \nu_i^2 / 2}
\end{equation}

$\phi$ has a maximum at

\begin{equation}
\hat\phi_{n, i} \propto e^{\lambda_i}\beta_{i, w_n} \label{eq:phiopt}
\end{equation}

($\phi$ then has to be normalised)

For maximisation with respect to $\lambda$ and $\nu^2$, numerical methods are used with derivatives

\begin{align}
dL/d\lambda & = -\Sigma^{-1}(\lambda - \mu) + \sum\limits_{n=1}^N\phi_{n, 1:K} - (N/\zeta)e^{\lambda + \nu^2/2} \\
dL/d\nu^2 & = -\mathit{diag}(\Sigma^{-1})/2 - \frac{N}{2\zeta}e^{\lambda + \nu^2/2} + \frac{1}{2\nu^2}
\end{align}

Blei's paper suggests using the conjugate gradient algorithm to optimise $\lambda$ and Newton's method for each coordinate of $\nu^2$, however, I found that I got better performance (both in convergence rates and the speed of the model) when using the limited-memory Broyden-Fletcher-Goldfarb-Shanno algorithm with box constraints (L-BFGS-B) to optimise for the whole vector $\nu^2$. I didn't write my own implementations of the conjugate gradient or the L-BFGS-B algorithm: they were available as a part of SciPy.

Once these variational parameters have been found, they are used to estimate the global model parameters:

\begin{align}
\hat\beta_i & \propto \sum\limits_d\phi_{d, i}n_d \\ \label{eq:betaopt}
\hat\mu & = \frac{1}{D} \sum\limits_d\lambda_d \\
\hat\Sigma & = \frac{1}{D} \sum\limits_d I\nu^2_d + (\lambda_d - \hat\mu)(\lambda_d - \hat\mu)^T
\end{align}

\subsection{Final algorithm of the training process}

The overall training process is as follows: set some initial model parameters and use those to perform variational inference on every document. Use the variational parameters to update the model parameters and repeat again until the likelihood bound converges (fractional change is less than a certain threshold).

\input{variational_inference}

\section{Incorporating priors into the Model}

In the training process, from \eqref{eq:phiopt} and \eqref{eq:betaopt} it follows that a zero entry in the $\beta$ matrix that the training process is initialized with will carry through to the $\phi$ matrices for every document and will be fed back into the updated $\beta$, thus enforcing that the inferred per-topic word distributions have zeros in the set places. This has the effect of setting pathway-gene membership priors and ensures that the inferred topic structure is referring to the actual pathways, so the results of the inference process will allow us to make judgements about real-world phenomena.

\section{Classification process}

In this case we aim to find, for every document, a $\theta$. Recall that $\theta$ is just a normalised $\eta$, so it's sufficient to estimate that.



TODO: read up on how theta is inferred

\section{Implementation in code and code optimisation}

To implement the training process for the model, I used Python, perhaps an unusual choice given the large amount of numerical computation that needs to be done and the poor performance of Python on these kinds of tasks. However, Python allows for extremely dense and expressive code, as well as has a REPL (Read-Eval-Print Loop), which allows for quick prototyping, implementation and debugging. The unsuitability of Python for numerical calculations is fixed by NumPy, a linear algebra library, that allows fast matrix operations in native code (using either routines provided by NumPy or more advanced numerical libraries that support the Basic Linear Algebra Subsystem (BLAS) API, such as ATLAS or OpenBLAS). In addition, NumPy integrates well with SciPy, a scientific computing library that provides useful routines, for example, for working with probability distributions or numerical optimisation.

Despite using NumPy, I still had performance issues which would have made working with the actual KEGG/CMap datasets very inconvenient. The training time for 1/10th of the dataset was about 3-4 hours, which implies more than 30-40 hours for the full dataset, since increasing its size would worsen the convergence rate. Using the profiler, I identified several performance bottlenecks in the code.

Firstly, the EM process used explicit Python list comprehensions that are evaluated using the Python interpreter, which I converted into matrix products.

Another major bottleneck is the likelihood bound calculation, which has to be performed every variational inference cycle for every document, since the termination criterion is based on the magnitude of changes in the bound. One solution to this would be not calculating the likelihood bound every iteration, but this resulted in worse performance: another variational inference cycle was more expensive than checking the bound. I used \texttt{numexpr}, a package that compiles operations on \texttt{NumPy} arrays into its own internal virtual machine bytecode and supports multithreaded computations on arrays.

The variational inference process requires calling the \texttt{SciPy} function minimizer for some parameters multiple times, which requires an initial value to start its search from. It turned out that in the beginning of the variational inference (every EM iteration) the initial values were the default ones. Recycling the variational parameters to continue the search from the previous iteration gave yet another performance improvement.

I also recompiled my \texttt{NumPy} distribution to use the Intel Math Kernel Library as its backend. Free for academic and personal use, Intel MKL provides a linear algebra library that implements the BLAS (Basic Linear Algebra Subsystem) API, optimized for manycore Intel CPUs.

Since the variational inference process is independent for every document, it can be parallelized across multiple cores. However, because of the Python Global Interpreter Lock, only one thread can execute Python bytecode at a time. This meant that I had to launch several Python processes if I wanted to take advantage of my CPU's capabilities. While this did provide an increase in CPU usage, it resulted in worse performance than using \texttt{numxepr} and Intel MKL's natural multithreading capabilities.

These efforts brought the training time down to about 8-9 hours for the whole dataset.

\chapter{Evaluation}

There are two ways the evaluation of the model can be performed: firstly, it can be trained on real-world gene expression and pathway membership data and evaluated against another dataset, called CTD, that lists, for every drug, the pathways that it does affect. However, it is not at all certain that the drug gene expression data actually does follow the generative framework defined by the CTM. Therefore, if model had poor performance on the real-world dataset, it would be impossible to find out whether it's because of an error in the implementation of the model or simply because the CTM is not suited to such tasks.

Hence, most of the evaluation was performed on toy datasets, generated as per the CTM's framework. This allowed to explore the capabilities of the CTM as well as employ more effective evaluation methods, since the model parameters and the per-document topic distributions that the corpus was generated with were available.

One important point about the evaluation of these kinds of models that should be made is that all the values and distributions that will be obtained as a result of all the methods here will have no meaning unless they can be compared to some sort of a baseline. Random guessing of topic proportions and model parameters provides a good baseline and so its results will feature throughout the analysis.

\section{Evaluation methods}

\subsection{Generating toy datasets}

The generative process for a toy dataset proceeds as follows:

\begin{itemize}[noitemsep]
\item Sample $\mu$ from a $K$-dimensional uniform distribution
\item Sample $\beta$ from $K$ $V$-dimensional uniform distributions and normalize it
\item Sample $\Sigma$ from an inverse Wishart distribution
\item Generate $D$ random documents
\end{itemize}

To generate a single document:

\begin{itemize}[noitemsep]
\item Sample $\eta_d$ from $N(\mu, \Sigma)$ and normalize it to get the topic distribution for the document, $\theta_d = \frac{e^{\eta_d}}{\sum{e^{\eta_d}}}$
\item Repeat $W$ times where $W$ is the number of words in each document:
\begin{itemize}[noitemsep]
\item Sample $z_{d, n}$, the topic the word belongs to, from $\eta_d$
\item Sample $w_{d, n}$ from $\beta{z_{d, n}}$, the word distribution for that topic
\end{itemize}
\end{itemize}

$W$ is chosen to be suffciently large as to provide a good approximation to the distribution of words in each document, since this is what essentially is being sampled.

The generation process returns the model parameters, the counts of words in each document, as well as the topic distribution for every document. Only the word counts are fed into the model: the other outputs are used for evaluation.

\subsection{Issues with topic recovery}

One big problem in the evaluation of the model is topic identifiability: the recovered topics are not at all guaranteed to be in the correct order and if they are close together, they are not guaranteed to be recovered. This means that even if the model gave perfect predictions about every document, it could score poorly simply because a topic that has one index in the model is referred to by a different index in the evaluation dataset. This can affect all parameters of the model, since all of them reference topics in some way ($\mu$ is the mean proportion of topics, $\Sigma$ is the covariance matrix of the topic proportions and $\beta$ is the topic-word distribution). This can be solved in two ways.

Firstly, if we're evaluating the model on toy datasets, we have access to the actual matrix of topic-word distributions $\beta$. Hence, a similarity matrix between the inferred and the reference $\beta$ can be constructed:

\begin{equation}
M_{i,j} = \frac{\beta^{inf}_i \cdot \beta^{ref}_j}{|\beta^{inf}_i||\beta^{ref}_j|}
\end{equation}

By inspecting this matrix, we can find out which topics from the inferred and the reference datasets are the closest and, if they form a permutation (so that the closest reference topic is unique for each of the inferred topics), we can construct a ``map" and permute the inferred parameters back accordingly:

\begin{align*}
&\mu'_{map_i} = \mu_i \\
&\Sigma'_{map_i, map_j} = \Sigma_{i, j} \\
&\beta'_{map_i, w} = \beta_{i, w} \\
&\theta'_{map_d} = \theta_i
\end{align*}

While this method is used when evaluating the recovery of the model parameters, it will not help with evaluation on the real-world dataset (since the actual parameters are, obviously, unavailable).

The second method is dealing with the root cause of this problem: that the topics are too close together. The real-world dataset is very sparse, both in terms of topic-word ($\beta$) and document-topic ($\theta$) relationships, so we can enforce this sparsity during the generation of the toy datasets (sparsity here refers to the fraction of zero elements in a given matrix).

To perform that, when the relevant vector is being generated, first, a sample is drawn from $\mathit{Poisson}((1-\rho) \times L)$ where $\rho$ is the density and $L$ is the length of the vector ($V$, the number of words, in case of $\beta$ or $K$, the number of topics, in case of $\theta$). This value is then clamped between 0 and $L$ and that number of smallest items in the vector is set to 0. Finally, the vector is renormalised.

The model is trained with the resultant pattern of zeros in $\beta$ set as its prior, which improves the chances of a successful topic recovery, hence making permuting the parameters based on the inferred-reference $\beta$ similarity matrix redundant (the matrix will have the greatest items on the diagonal). This can be considered cheating, but since the real-world gene-pathway membership dataset is very sparse as well, this is a necessary measure.

\subsection{Topic prediction}

Since the final output of the model is a distribution of topics for every document, it makes sense to evaluate the model based on that.

The performance measure that is not affected by the topic identifiability is the document similarity matrix, which is very close in its definition to the beta matrix used to permute the parameters. Given the inferred and the reference $\theta$, the per-document topic distribution, we can take their cosine similarity to see how similar any two documents are. Thus, for the whole corpus, we can construct a document similarity matrix and the two matrices for the inferred and the reference distributions can be compared:

\begin{equation}
M_{i,j} = \frac{\theta_i \cdot \theta_j}{|\theta_i||\theta_j|}
\end{equation}

Similarly, if we've ensured that the topics were recovered correctly (through permuting the parameters or having sufficient sparsity in the dataset), the cosine similarities can be taken between inferred and reference $\theta$ for every document and a histogram of these values plotted to evaluate the topic prediction.

\subsection{Parameter recovery}

Again assuming the topic recovery, the inferred model parameters can be compared to the reference parameters that the dataset was generated with.

First, we turn the parameters into a ``canonical" form by normalising them:

\begin{equation}
\mu^{norm}_i = \frac{e^{\mu_i}}{\sum\limits_{i=1}^K{e^{\mu_i}}}
\end{equation}

The covariance matrix $\Sigma$ is turned into the correlation matrix so that the correlations inferred by the model and the reference correlations can be compared:

\begin{equation}
C_{i, j} = \frac{|\Sigma^{-1}_{i, j}|}{\sqrt{\Sigma^{-1}_{i, i}\Sigma^{-1}_{j, j}}}
\end{equation}

The parameters are then compared to the reference using the normalized root-mean-square error (RMSE):

\begin{equation}
\mathit{RMSE}(I, R) = \frac{\sqrt{\sum{(I-R)^2}}}{\sqrt{D}(max(R) - min(R))}
\end{equation}

where $I$ is the inferred parameter, $R$ is the reference parameter and $D$ is the size of the matrix/vector.

TODO: add about graph recovery (correlation matrix)

\subsection{Rank recovery}

This method is used to validate the model against the real-world dataset (CTD) that does not contain actual drug-pathway distributions: instead, for every drug, it lists the pathways that it does affect. This is the rationale behind adding support for enforcing the sparsity of the topic matrices: that way datasets with drugs that only affect a few pathways can be generated and so this method can also be used when working with generated datasets.

The problem of evaluating pathway distributions against a list of relevant pathways strikingly resembles a similar problem from information retrieval: given a ranking for documents (here, pathways) output by a model for various queries (here, drug gene expression profiles) and a judgement as to which documents (pathways) are relevant (actually affected by the drug), how can evaluation be performed?

Two metrics that are used in information retrieval to evaluate such systems are precision and recall: precision is the fraction of retrieved documents that are relevant and recall is the fraction of relevant documents that are retrieved. These metrics can be calculated at all the cutoff ranks in the output pathway list and used to plot a precision-recall curve.

Another way of evaluating the distributions I came up with is, for every drug and for every pathway that this drug actually affects, summing the probabilities predicted by the model for that pathway. This results in a value for every drug that represents, in essence, the fraction of drug-pathway activity that the model got ``right":

\begin{equation}
c_d = \sum\limits_{p \in \mathit{ref}_d}{\theta_{d, p}}
\end{equation}

where $\mathit{ref}_d$ is a set of correct pathways for every drug and $\theta_d$ is the predicted pathway distribution for drug $d$.

The distribution of these values can then be compared to the performance of random guessing to judge whether the model gives significantly different results.

\section{Toy datasets}

The following subsections describe the datasets which were generated and describes the results of the evaluation as per the aforementioned methods

\subsection{Description of toy datasets}

The datasets were generated so that they mimic the properties of the real world KEGG/CMap dataset. All datasets have the following in common:

\begin{itemize}[noitemsep]
\item Vocabulary length (number of genes): 3000
\item Number of documents (drugs): 500
\item Number of words in each document: 1000
\item Density of $\beta$: 0.0138
\item Density of $\theta$: 0.1231
\end{itemize}

Densities here refer to the fraction of non-zero elements in each matrix. The KEGG/CMap dataset is very sparse: each pathway only affects, on average 1.38\% of all genes and each drug only affects, on average, 12.3\% of all pathways. The density value for $\beta$ was calculated by taking the matrix of priors (since it explicitly enforces the pattern of zeros in the result), whereas the density value for $\theta$ was calculated by taking the reference (CTD) dataset.

With these parameters, 5 datasets were generated:

\begin{itemize}[noitemsep]
\item 10 topics
\item 20 topics
\item 50 topics
\item 100 topics
\end{itemize}

These datasets allow to investigate how the performance of the model changes when the number of topics increases.

The parameters of the fifth dataset were designed to mimic the real-world one as closely as possible in order to determine, by comparing the performance of the model on that dataset against the real-world one, whether the drug gene expression data follows the generative framework described by the CTM. In addition to reusing all the parameters the first 4 datasets were generated with, for this dataset I also approximated the $\mu$ and $\Sigma$ of the reference by first calculating the topic distribution $\theta$ implied by the dataset (assuming all pathways in the resultant matrix are equally weighted) and then calculating the mean and the covariance matrix of the distribution of pathways based on it.

In the last 4 datasets, I varied the density of the covariance matrix: the proportion of pathways in the implied matrix of correlations that are connected:

\begin{itemize}[noitemsep]
\item No correlations: use the identity matrix as $\Sigma$
\item Density 10\%
\item Density 50\%
\item Density 90\%
\end{itemize}

\subsection{Convergence analysis}

At every iteration, the model outputs a likelihood bound that it assigns to the data and continues updating the model parameters until the likelihood bound converges (the change is less than a certain threshold). The likelihood bound values over iterations are plotted here.

It can be observed that increasing the number of topics decreases the likelihood bound the model assigns to the data and makes convergence slower.

\subsection{Topic prediction}

\subsection{Parameter recovery}

\subsection{Rank recovery}

\section{KEGG and CMap datasets}

\subsection{Description of the dataset}

KEGG (Kyoto Encyclopedia of Genes and Genomes), published by Kyoto Universiy Katehisa Laboratories, is an online collection of databases dealing with various aspects of biology. Of interest to this project is the KEGG PATHWAY database, a collection of pathways and genes which they include. CMap (Connectivity Map), published by the Broad Institute is a catalog of gene-expression data.

These two datasets are used to derive the following matrices:

\begin{itemize}[noitemsep]
\item Drug-gene expression data, where each entry is the logarithm of the ratio of the expression of the gene when treated with a certain drug against the baseline, rounded to 2 decimal figures.
\item Boolean gene-pathway membership matrix that has a ``1" entry if the pathway contains a given gene and ``0" if it doesn't.
\end{itemize}

I used the same preprocessed data that was used in the LDA paper, so I didn't have to perform the preprocessing of the raw gene expression data myself. This was so that the results from the LDA and the CTM method could be compared.

The parameters of the training dataset are:
\begin{itemize}[noitemsep]
\item 3041 gene
\item 236 pathways
\item 1169 drugs
\item Density of $\beta$: 0.0138
\item Density of $\theta$: 0.1231
\end{itemize}

It took about 6 hours to train the model on the whole dataset.

Before the CTM can be trained on the dataset, the drug-gene expression data has to be turned into a sort of a ``document", since we're now operating in terms of words and word counts. This is performed by taking the absolute value of each element and multiplying it by 100 (since the preprocessed data has been rounded to the nearest 0.01). Observe that this discards the sign of the expression value. Since we're operating in log space, this means that the gene expression ratios that are reciprocals of each other will be treated the same.

\subsection{CTD}

\subsubsection{Precision-recall curves}
Side plot from LDA, heatmap of correctly predicted pathways, PR curves (+ vs random)

\subsubsection{Fraction of correctly identified pathway activity}
The CDF thing (LDA-CTM-RND)

\subsubsection{Cool pictures from the dataset}
Cluster pathways/drugs, some diagrams and topic graphs, compare with the LDA results, compare with the known drug pathways.

\subsection{ATC classification}

Drugs have a hierarchical code that depends on what they actually do (example). This code is possible to predict from the gene expression data by training a generic classifier on it.

If the inferred pathway proportions from the models are used instead as inputs, is the performance better?

\subsection{Drug-target prediction}

\chapter{Conclusion}

%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
% the bibliography
\addcontentsline{toc}{chapter}{Bibliography}
\bibliography{bibliography}

%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
% the appendices
\appendix

\chapter{Project Proposal}

\input{proposal}

\end{document}
