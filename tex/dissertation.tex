\documentclass[12pt,a4paper,twoside,openright]{report}
\usepackage[pdfborder={0 0 0}]{hyperref}    % turns references into hyperlinks
\usepackage[margin=25mm]{geometry}  % adjusts page layout
\usepackage{graphicx}  % allows inclusion of PDF, PNG and JPG images
\usepackage{verbatim}
\usepackage{docmute}   % only needed to allow inclusion of proposal.tex

\raggedbottom                           % try to avoid widows and orphans
\sloppy
\clubpenalty1000%
\widowpenalty1000%

\renewcommand{\baselinestretch}{1.1}    % adjust line spacing to make
                                        % more readable

\begin{document}

\bibliographystyle{plain}


%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
% Title


\pagestyle{empty}

\rightline{\LARGE \textbf{Artjoms Iskovs}}

\vspace*{60mm}
\begin{center}
\Huge
\textbf{Predicting drug-pathway interactions using the Correlated Topic Model} \\[5mm]
Computer Science Tripos -- Part II \\[5mm]
Trinity College \\[5mm]
\today  % today's date
\end{center}

%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
% Proforma, table of contents and list of figures

\pagestyle{plain}

\chapter*{Proforma}

{\large
\begin{tabular}{ll}
Name:               & \bf Artjoms Iskovs                       \\
College:            & \bf Trinity College                     \\
Project Title:     & \bf Predicting drug-pathway interactions using the Correlated Topic Model\\
Examination:        & \bf Computer Science Tripos -- Part II, July 2015  \\
Word Count:         & \bf ??\\
Project Originator: & N.~Pratanwanich                    \\
Supervisor:         & N.~Pratanwanich                    \\ 
\end{tabular}
}

\section*{Original Aims of the Project}
Implement the Correlated Topic Model for the problem of inferring pathways from drug gene expression data. Find a way to enforce gene-pathway membership priors so that the recovered topic structure can be related to the real data.

\section*{Work Completed}

\section*{Special Difficulties}
None
 
\newpage
\section*{Declaration}

I, Artjoms Iskovs of Trinity College, being a candidate for Part II of the Computer
Science Tripos, hereby declare that this dissertation and the work described in it are my own work,
unaided except as may be specified below, and that the dissertation does not contain material that has already been used to any substantial
extent for a comparable purpose.

\bigskip
\leftline{Signed [signature]}

\medskip
\leftline{Date [date]}

\tableofcontents

\listoffigures

\newpage
\section*{Acknowledgements}


%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
% now for the chapters

\pagestyle{headings}

\chapter{Introduction}

Topic models come from the realm of natural language processing and are bag-of-words models that allow for exploratory data analysis on collections of documents. They assume that documents in a corpus are generated by first drawing a distribution of topics for every document, then picking words for a document by first choosing a topic from this distribution and then sampling a word from that distribution.

Rationale for applying these to drug modelling: can quicker reject useless drugs, can find new uses (pathways) for current drugs, the pathway correlation matrix is useful.

\chapter{Preparation}

\chapter{Implementation}

\section{Training process}

Probably a feedback picture with variational parameters and model parameters here.
Copy the pseudocode over as well.

The core of the training process is optimizing the likelihood bound on the model: how likely is the corpus given the model. By altering the parameters of the model in order to maximize this bound, the model is trained on the data.

The training happens in cycles. First, a variational distribution is fit to every document in the corpus. Then, the new model parameters are inferred and the likelihood bound recalculated. This process repeats until changes in the likelihood bound are negligible.

\section{Incorporating priors into the Model}

In the training process, observe that:

This means that a zero entry in the $\beta$ matrix that the training process is initialized with will carry through to the variational parameters for every document and will be fed back into the updated $\beta$, thus enforcing that the inferred per-topic word distributions have zeros in the set places. This has the effect of setting pathway-gene membership priors and ensures that the inferred topic structure is referring to the actual pathways, so the results of the inference process will allow us to make judgements about real-world phenomena.

\section{Classification process}

TODO: read up on how theta is inferred

\section{Implementation in code}

Python!

\section{Code optimization}

Since Python is an interpreted language, it is great for quick prototyping and implementation. Unfortunately, it means that its performance is several orders of magnitude is several orders of magnitude below that of C code. Using NumPy for numerical calculations definitely helps, since the matrix product operations are performed in native code, but I still had performance issues which would have made working with the actual KEGG/CMap datasets very inconvenient. The training time for 1/10th of the dataset was about 3-4 hours, which implies more than 30-40 hours for the full dataset, since increasing its size would worsen the convergence rate. Using the profiler, I identified several performance bottlenecks in the code.

Firstly, the EM process used explicit list comprehensions -> converted into dot products
Secondly, the likelihood bound calculations -> used numexpr for those
Thirdly, variational parameters re-inferred every cycle -> seeding the optimizer with the previous results, problem: not enough memory
Also, precalculating some often-used values (inverse sigma, sparse phi etc)
Also, describe using Python multiprocessing vs Intel MKL and conflicts between them (+ Python GIL)

These efforts brought the training time down to about 8-9 hours for the whole dataset.

\chapter{Evaluation}

\section{Toy datasets}

Since it is difficult to verify correctness of the model on the real drug dataset, I had to write some routines for toy dataset generation of various complexities.

\begin{itemize}
\item Sample $\mu$ from a K-dimensional uniform distribution
\item Sample $\beta$ from K voclen-dimensional uniform distributions and normalize it
\item Sample $\Sigma$ from an inverse Wishart distribution
\item Generate N random documents
\end{itemize}

The generation process returns the model parameters, the counts of words in each document, as well as the topic distribution for every document. Only the word counts are fed into the model: the other outputs are used for evaluation.

The big problem in the evaluation of the model is topic identifiability: the recovered topics are not at all guaranteed to be in the correct order and if they are close together, they are not guaranteed to be recovered. To combat this, the entries on the diagonal of the $\beta$ matrix are set to zero: that way every topic has a missing word.

The performance measure that is not affected by the topic identifiability is the document similarity matrix. Given the inferred and the reference $\theta$, the per-document topic distribution, we can take their cosine similarity to see how similar any two documents are. Thus, for the whole corpus, we can construct a document similarity matrix and the two matrices for the inferred and the reference distributions can be compared.

To see how well the topic structure has been recovered, a similar matrix can be constructed for $\beta$, the per-topic word distribution. Using this matrix, we can see which topics from the inferred structure are the closest to the original ones and permute the results of the model accordingly, giving us access to more evaluation measures.

Perhaps vary K and the vocabulary length to plot a grid of RMSEs, also add the CDF from some sample tests.

\section{KEGG and CMap datasets}

Some diagrams and topic graphs here, compare with the LDA results, compare with the known drug pathways.

\chapter{Conclusion}


%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
% the bibliography
\addcontentsline{toc}{chapter}{Bibliography}
\bibliography{bibliography}

%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
% the appendices
\appendix

\chapter{Project Proposal}

\input{proposal}

\end{document}
