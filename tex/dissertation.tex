\documentclass[12pt,a4paper,twoside,openright]{report}
\usepackage[pdfborder={0 0 0}]{hyperref}    % turns references into hyperlinks
\usepackage[margin=25mm]{geometry}  % adjusts page layout
\usepackage{graphicx}  % allows inclusion of PDF, PNG and JPG images
\usepackage{verbatim}
\usepackage{docmute}   % only needed to allow inclusion of proposal.tex
\usepackage[noend]{algpseudocode}
\usepackage[]{algorithm}
\usepackage{amsmath}
\newcommand{\var}[1]{{\operatorname{\mathit{#1}}}}

\raggedbottom                           % try to avoid widows and orphans
\sloppy
\clubpenalty1000%
\widowpenalty1000%

\renewcommand{\baselinestretch}{1.1}    % adjust line spacing to make
                                        % more readable

\begin{document}

\bibliographystyle{plain}


%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
% Title


\pagestyle{empty}

\rightline{\LARGE \textbf{Artjoms Iskovs}}

\vspace*{60mm}
\begin{center}
\Huge
\textbf{Predicting drug-pathway interactions using the Correlated Topic Model} \\[5mm]
Computer Science Tripos -- Part II \\[5mm]
Trinity College \\[5mm]
\today  % today's date
\end{center}

%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
% Proforma, table of contents and list of figures

\pagestyle{plain}

\chapter*{Proforma}

{\large
\begin{tabular}{ll}
Name:               & \bf Artjoms Iskovs                       \\
College:            & \bf Trinity College                     \\
Project Title:     & \bf Predicting drug-pathway interactions using the Correlated Topic Model\\
Examination:        & \bf Computer Science Tripos -- Part II, July 2015  \\
Word Count:         & \bf ??\\
Project Originator: & N.~Pratanwanich                    \\
Supervisor:         & N.~Pratanwanich                    \\ 
\end{tabular}
}

\section*{Original Aims of the Project}
Implement the Correlated Topic Model for the problem of inferring pathways from drug gene expression data. Find a way to enforce gene-pathway membership priors so that the recovered topic structure can be related to the real data.

\section*{Work Completed}

\section*{Special Difficulties}
None
 
\newpage
\section*{Declaration}

I, Artjoms Iskovs of Trinity College, being a candidate for Part II of the Computer
Science Tripos, hereby declare that this dissertation and the work described in it are my own work,
unaided except as may be specified below, and that the dissertation does not contain material that has already been used to any substantial
extent for a comparable purpose.

\bigskip
\leftline{Signed [signature]}

\medskip
\leftline{Date [date]}

\tableofcontents

\listoffigures

\newpage
\section*{Acknowledgements}


%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
% now for the chapters

\pagestyle{headings}

\chapter{Introduction}

Drugs are often investigated by using gene expression microarray analysis: a matrix of various samples of DNA is treated with different drugs and their expression is compared with the baseline expression.

Functionally similar genes are grouped into pathways.

Topic models come from the realm of natural language processing and are bag-of-words models that allow for exploratory data analysis on collections of documents. They assume that documents in a corpus are generated by first drawing a distribution of topics for every document, then picking words for a document by first choosing a topic from this distribution and then sampling a word from that distribution.

Using topic models in order to predict pathways affected by drugs has many benefits. Firstly, pathways that a new drug is likely to affect can be predicted without having to perform tests on real people, which means that useless drugs can be rejected quicker. In addition, this kind of analysis can allow researchers to find new uses (affected pathways) for existing drugs. Finally, the inferred correlation matrix can provide useful insights into relationships between pathways (such as disease comorbidity).

\chapter{Preparation}

\chapter{Implementation}

\section{Correlated Topic Model}

Bayesian plate notation here + explanation

\section{Training process}

Quote the derivation from Blei's paper?

The training process for the model is Bayesian inference: we express P(data|model) and vary the model parameters in order to maximize this probability

Probably a feedback picture with variational parameters and model parameters here.
\input{variational_inference}

The core of the training process is optimizing the likelihood bound on the model: how likely is the corpus given the model. By altering the parameters of the model in order to maximize this bound, the model is trained on the data.

The training happens in cycles. First, a variational distribution is fit to every document in the corpus. Then, the new model parameters are inferred and the likelihood bound recalculated. This process repeats until changes in the likelihood bound are negligible.

\section{Incorporating priors into the Model}

In the training process, observe that:

This means that a zero entry in the $\beta$ matrix that the training process is initialized with will carry through to the variational parameters for every document and will be fed back into the updated $\beta$, thus enforcing that the inferred per-topic word distributions have zeros in the set places. This has the effect of setting pathway-gene membership priors and ensures that the inferred topic structure is referring to the actual pathways, so the results of the inference process will allow us to make judgements about real-world phenomena.

\section{Classification process}

TODO: read up on how theta is inferred

\section{Implementation in code}

Python!

\section{Code optimization}

Since Python is an interpreted language, it is great for quick prototyping and implementation. Unfortunately, it means that its performance is several orders of magnitude below that of C code. Using NumPy for numerical calculations definitely helps, since the matrix product operations are performed in native code, but I still had performance issues which would have made working with the actual KEGG/CMap datasets very inconvenient. The training time for 1/10th of the dataset was about 3-4 hours, which implies more than 30-40 hours for the full dataset, since increasing its size would worsen the convergence rate. Using the profiler, I identified several performance bottlenecks in the code.

Firstly, the EM process used explicit Python list comprehensions that are evaluated using the Python interpreter, which I converted into matrix products.

Another major bottleneck is the likelihood bound calculation, which has to be performed every variational inference cycle for every document, since the termination criterion is based on the magnitude of changes in the bound. One solution to this would be not calculating the likelihood bound every iteration, but this resulted in worse performance: another variational inference cycle was more expensive than checking the bound. I used \texttt{numexpr}, a package that compiles operations on \texttt{NumPy} arrays into its own internal virtual machine bytecode and supports multithreaded computations on arrays.

The variational inference process requires calling the \texttt{SciPy} function minimizer for some parameters multiple times, which requires an initial value to start its search from. It turned out that in the beginning of the variational inference (every EM iteration) the initial values were the default ones. Recycling the variational parameters to continue the search from the previous iteration gave yet another performance improvement.

I also recompiled my \texttt{NumPy} distribution to use the Intel Math Kernel Library as its backend. Free for academic and personal use, Intel MKL provides a linear algebra library that implements the BLAS (Basic Linear Algebra Subsystem) API, optimized for manycore Intel CPUs.

Since the variational inference process is independent for every document, it can be parallelized across multiple cores. However, because of the Python Global Interpreter Lock, only one thread can execute Python bytecode at a time. This meant that I had to launch several Python processes if I wanted to take advantage of my CPU's capabilities. While this did provide an increase in CPU usage, it resulted in worse performance than using \texttt{numxepr} and Intel MKL's natural multithreading capabilities.

These efforts brought the training time down to about 8-9 hours for the whole dataset.

\chapter{Evaluation}

\section{Toy datasets}

While the model could be used to predict real-world drug-pathway interactions, it is not at all certain that they actually do follow the generative framework defined by the CTM. Hence, most of the evaluation of the model was done on generated datasets which allowed to verify the model actually corresponds to the specification as well as explore the capabilities of the CTM.

The generative process for a toy dataset proceeds as follows:

\begin{itemize}
\item Sample $\mu$ from a K-dimensional uniform distribution
\item Sample $\beta$ from K voclen-dimensional uniform distributions and normalize it
\item Sample $\Sigma$ from an inverse Wishart distribution
\item Generate N random documents
\end{itemize}

To generate a single document:

\begin{itemize}
\item Sample $\eta_d$ from $Norm(\mu, \Sigma)$ and normalize it to get the topic distribution for the document, $\theta_d = \frac{e^{\eta_d}}{\sum{e^{\eta_d}}}$
\item Repeat W times where W is the number of words in each document:
\begin{itemize}
\item Sample $z_{d, n}$, the topic the word belongs to, from $\eta_d$
\item Sample $w_{d, n}$ from $\beta{z_{d, n}}$, the word distribution for that topic
\end{itemize}
\end{itemize}

W is chosen to be suffciently large as to provide a good approximation to the distribution of words in each document, since this is what essentially is being sampled.

The generation process returns the model parameters, the counts of words in each document, as well as the topic distribution for every document. Only the word counts are fed into the model: the other outputs are used for evaluation.

The following subsections go into detail about which methods of evaluation were performed.

\subsection{Evaluating the topic prediction}

Since the final output of the model is a distribution of topics for every document, it makes sense to evaluate the model based on that. However, the big problem in the evaluation of the model is topic identifiability: the recovered topics are not at all guaranteed to be in the correct order and if they are close together, they are not guaranteed to be recovered. This means that even if the model gave perfect predictions about every document, it could score poorly simply because a topic that has one index in the model is referred to by a different index in the evaluation dataset.

The performance measure that is not affected by the topic identifiability is the document similarity matrix. Given the inferred and the reference $\theta$, the per-document topic distribution, we can take their cosine similarity to see how similar any two documents are. Thus, for the whole corpus, we can construct a document similarity matrix and the two matrices for the inferred and the reference distributions can be compared.

To gain access to more evaluation measures, a similar matrix can be constructed for $\beta$, the per-topic word distribution. Using this matrix, we can see which topics from the inferred structure are the closest to the original ones and permute the results of the model accordingly. However, this doesn't help with some generated topics being close together and so the generated dataset actually has some entries in the word-topic matrix set to zero. In addition, the model is trained with the pattern of zeros set as its prior, which improves the chances of a successful topic recovery. While this can be considered cheating, the real-world gene-pathway membership dataset is, in fact, very sparse as well (for every pathway, most genes don't participate in it), so this is a necessary measure.

\subsection{Evaluating the parameter recovery}

Perhaps vary K and the vocabulary length to plot a grid of RMSEs, also add the CDF from some sample tests.
Sigma: topic structure (graph) recovery

\subsection{Rank recovery}

The validation dataset (CTD) does not contain actual drug-pathway distributions: instead, for every drug, it lists the pathways that it does affect. Therefore, the toy dataset generation method had to be altered: when the topic distribution for a drug is drawn, a certain number of lowest-represented pathways is set to zero, enforcing that every drug, in essence, affects only a few pathways.

The problem of evaluating pathway distributions against a list of relevant pathways strikingly resembles a similar problem from information retrieval: given a ranking for documents (here, pathways) output by a model for various queries (here, drug gene expression profiles) and a judgement as to which documents (pathways) are relevant (actually affected by the drug), how can evaluation be performed?

(MAP/ranks here, side plots)

Another way of evaluating the distributions I came up with is, for every drug and for every pathway that this drug actually affects, summing the probabilities predicted by the model for that pathway. This results in a value for every drug that represents, in essence, the fraction of drug-pathway activity that the model got ,,right". The distribution of these values can then be compared to the performance of random guessing to judge whether the model gives significantly different results.

\section{KEGG and CMap datasets}

What are KEGG/CMap?

Some diagrams and topic graphs here, compare with the LDA results, compare with the known drug pathways.

Cluster pathways/drugs

Side plot from LDA, heatmap of correctly predicted pathways

\chapter{Conclusion}

%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
% the bibliography
\addcontentsline{toc}{chapter}{Bibliography}
\bibliography{bibliography}

%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
% the appendices
\appendix

\chapter{Project Proposal}

\input{proposal}

\end{document}
